{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features import delta\n",
    "from python_speech_features import logfbank\n",
    "import scipy.io.wavfile as wav\n",
    "from random import randint\n",
    "# import librosa\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_array = os.listdir(\"/home/rohan/BE_project/dataset/enroll\");\n",
    "\n",
    "directory = os.path.join(os.getcwd(), 'dataset/enroll')\n",
    "\n",
    "for file_name in file_array :\n",
    "#/home/punit/Downloads/test/{}  directory which contains raw files \n",
    "#/home/punit/Downloads/processed/{} directory which contains processed files\n",
    "#install sox before the script is run. \n",
    "#Meaning of parameters refer this article https://digitalcardboard.com/blog/2009/08/25/the-sox-of-silence/\n",
    "#     os.system(\"rm -f /home/rohan/BE_project/raw/enroll_processed/*\")\n",
    "# 0.3 -70d -1 0.1 1% - 57\n",
    "#     cmd = \"sox /home/rohan/BE_project/raw/enroll/{} /home/rohan/BE_project/raw/enroll/noise-{} trim 0 0.900\".format(file_name, file_name)\n",
    "#     os.system(cmd);\n",
    "#     cmd = \"sox /home/rohan/BE_project/raw/enroll/noise-{} -n noiseprof /home/rohan/BE_project/raw/enroll/noise-{}.prof\".format(file_name, file_name)\n",
    "#     os.system(cmd);\n",
    "#     cmd = \"sox /home/rohan/BE_project/raw/enroll/{} /home/rohan/BE_project/raw/temp/{} rate 44100\".format(file_name, file_name, file_name)\n",
    "#     os.system(cmd);\n",
    "\n",
    "    (rate,sig) = wav.read(os.path.join(directory, file_name))\n",
    "    if int(rate) >= 8000:\n",
    "        cmd = \"sox /home/rohan/BE_project/dataset/enroll/{} /home/rohan/BE_project/dataset/enroll_processed/{} silence 1 0.3 -55d -1 0.1 1% lowpass 7000\".format(file_name, file_name)\n",
    "        os.system(cmd);\n",
    "    else:\n",
    "        cmd = \"sox /home/rohan/BE_project/dataset/enroll/{} /home/rohan/BE_project/dataset/enroll_processed/{} silence 1 0.3 -55d -1 0.1 1% lowpass 3500\".format(file_name, file_name)\n",
    "        os.system(cmd);\n",
    "#     cmd = \"sox /home/rohan/BE_project/raw/enroll_processed/{} /home/rohan/BE_project/raw/enroll_processed/{} lowpass 20000\".format(file_name, file_name)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# file_array = os.listdir(\"/home/rohan/BE_project/raw/test\");\n",
    "# for file_name in file_array :\n",
    "# #/home/punit/Downloads/test/{}  directory which contains raw files \n",
    "# #/home/punit/Downloads/processed/{} directory which contains processed files\n",
    "# #install sox before the script is run. \n",
    "# #Meaning of parameters refer this article https://digitalcardboard.com/blog/2009/08/25/the-sox-of-silence/\n",
    "# #     os.system(\"rm -f /home/rohan/BE_project/raw/test_processed/*\")\n",
    "#     cmd = \"sox /home/rohan/BE_project/raw/test/{} /home/rohan/BE_project/raw/test_processed/{} silence 1 0.3 -45d -1 0.1 1% lowpass 7000\".format(file_name, file_name)\n",
    "# #     print(cmd);\n",
    "#     os.system(cmd);\n",
    "\n",
    "    \n",
    "file_array = os.listdir(\"/home/rohan/BE_project/dataset/test\");\n",
    "\n",
    "directory = os.path.join(os.getcwd(), 'dataset/test')\n",
    "\n",
    "for file_name in file_array :\n",
    "#/home/punit/Downloads/test/{}  directory which contains raw files \n",
    "#/home/punit/Downloads/processed/{} directory which contains processed files\n",
    "#install sox before the script is run. \n",
    "#Meaning of parameters refer this article https://digitalcardboard.com/blog/2009/08/25/the-sox-of-silence/\n",
    "#     os.system(\"rm -f /home/rohan/BE_project/raw/enroll_processed/*\")\n",
    "# 0.3 -70d -1 0.1 1% - 57\n",
    "#     cmd = \"sox /home/rohan/BE_project/raw/enroll/{} /home/rohan/BE_project/raw/enroll/noise-{} trim 0 0.900\".format(file_name, file_name)\n",
    "#     os.system(cmd);\n",
    "#     cmd = \"sox /home/rohan/BE_project/raw/enroll/noise-{} -n noiseprof /home/rohan/BE_project/raw/enroll/noise-{}.prof\".format(file_name, file_name)\n",
    "#     os.system(cmd);\n",
    "#     cmd = \"sox /home/rohan/BE_project/raw/enroll/{} /home/rohan/BE_project/raw/temp/{} rate 44100\".format(file_name, file_name, file_name)\n",
    "#     os.system(cmd);\n",
    "\n",
    "    (rate,sig) = wav.read(os.path.join(directory, file_name))\n",
    "    if int(rate) >= 8000:\n",
    "        cmd = \"sox /home/rohan/BE_project/dataset/test/{} /home/rohan/BE_project/dataset/test_processed/{} silence 1 0.3 -55d -1 0.1 1% lowpass 7000\".format(file_name, file_name)\n",
    "        os.system(cmd);\n",
    "    else:\n",
    "        cmd = \"sox /home/rohan/BE_project/dataset/test/{} /home/rohan/BE_project/dataset/test_processed/{} silence 1 0.3 -55d -1 0.1 1% lowpass 3500\".format(file_name, file_name)\n",
    "        os.system(cmd);\n",
    "#     cmd = \"sox /home/rohan/BE_project/raw/enroll_processed/{} /home/rohan/BE_project/raw/enroll_processed/{} lowpass 20000\".format(file_name, file_name)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# every frame is considered independent\n",
    "\n",
    "no_of_features = 13\n",
    "no_of_fbank_features = 13\n",
    "no_of_columns = (3 * no_of_features) + no_of_fbank_features\n",
    "test_frames = 50\n",
    "classes = 30\n",
    "\n",
    "def get_feature_vectors(dataset_type):\n",
    "    \n",
    "    #set parameters for training and testing\n",
    "    if (dataset_type == \"train\"):\n",
    "#         directory = os.path.join(os.getcwd(), 'data_thuyg20_sre/enroll')\n",
    "        directory = os.path.join(os.getcwd(), 'dataset/enroll_processed')\n",
    "        no_of_frames = 800\n",
    "        start_frame = 10\n",
    "    elif (dataset_type == \"test\"):    \n",
    "#         directory = os.path.join(os.getcwd(), 'data_thuyg20_sre/test')\n",
    "        directory = os.path.join(os.getcwd(), 'dataset/test_processed')\n",
    "        no_of_frames = test_frames\n",
    "        start_frame = 1\n",
    "        \n",
    "    dataset = numpy.empty([0, no_of_columns + 1])\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        \n",
    "        # filter speakers\n",
    "        n = int(classes/2)\n",
    "#         print(n)\n",
    "        names = []\n",
    "        \n",
    "        for i in range (1, n+1):\n",
    "            names.append('F1' + '%02d'%(i))\n",
    "            names.append('M1' + '%02d'%(i))\n",
    "\n",
    "        if any(name in file for name in names):\n",
    "            \n",
    "            # extract mfcc vectors\n",
    "#             print(file)\n",
    "            (rate,sig) = wav.read(os.path.join(directory, file))\n",
    "            fbank_feat = logfbank(sig,rate,nfft=2048)\n",
    "            mfcc_feat = mfcc(sig,rate,winlen=0.032,winstep=0.016,numcep=13,nfft=2048)\n",
    "            \n",
    "#             print(\"SR :\" + str(rate) + \" \" + file)\n",
    "#             y, sr = librosa.load(os.path.join(directory, file))\n",
    "#             mfcc_feat = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, n_fft=2048).T\n",
    "            d_mfcc_feat = delta(mfcc_feat, 2)\n",
    "            dd_mfcc_feat = delta(d_mfcc_feat, 2)\n",
    "            \n",
    "#             fbank_feat = logfbank(sig,rate)\n",
    "            mfcc_vectors = mfcc_feat[start_frame:start_frame+no_of_frames,:no_of_features]\n",
    "            dmfcc_vectors = d_mfcc_feat[start_frame:start_frame+no_of_frames,:no_of_features]\n",
    "            ddmfcc_vectors = dd_mfcc_feat[start_frame:start_frame+no_of_frames,:no_of_features]\n",
    "            fbank_vectors = fbank_feat[start_frame:start_frame+no_of_frames,:no_of_fbank_features]\n",
    "            \n",
    "            feature_vectors = numpy.hstack((mfcc_vectors, dmfcc_vectors, ddmfcc_vectors, fbank_vectors))\n",
    "#             print(feature_vectors.shape)\n",
    "            \n",
    "            # get speaker index from filename\n",
    "            speaker_index = file.split(\"_\")[0]\n",
    "            if speaker_index[0] == 'M':\n",
    "                speaker_index = n + int(speaker_index[3:])\n",
    "            else:\n",
    "                speaker_index = int(speaker_index[3:])\n",
    "\n",
    "            #append speaker index to feature vectors\n",
    "            np_speaker_index = numpy.array([speaker_index])\n",
    "            temp = numpy.tile(np_speaker_index[numpy.newaxis,:], (feature_vectors.shape[0],1))\n",
    "            concatenated_feature_vector = numpy.concatenate((feature_vectors,temp), axis=1)\n",
    "            \n",
    "#             print(concatenated_feature_vector.shape)\n",
    "#             print(fbank_vectors.shape)\n",
    "            \n",
    "            # append file's data to dataset\n",
    "            dataset = numpy.concatenate((dataset, concatenated_feature_vector), axis=0)\n",
    "            \n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "import numpy as numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from numpy import genfromtxt\n",
    "my_data = get_feature_vectors(\"train\")\n",
    "numpy.random.shuffle(my_data)\n",
    "# get_feature_vectors(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 53)\n"
     ]
    }
   ],
   "source": [
    "# numpy.set_printoptions(threshold=numpy.nan)\n",
    "# print(my_data)\n",
    "print(my_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 1)\n"
     ]
    }
   ],
   "source": [
    "Y = numpy.copy(my_data[:, no_of_columns:])\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 52)\n",
      "(1, 52)\n",
      "(1, 52)\n",
      "(24000, 52)\n"
     ]
    }
   ],
   "source": [
    "X = numpy.copy(my_data[:, :no_of_columns])\n",
    "print(X.shape)\n",
    "mean = X.mean(0, keepdims=True)\n",
    "\n",
    "print(mean.shape)\n",
    "std_deviation = numpy.std(X, axis=0, keepdims=True)\n",
    "print(std_deviation.shape)\n",
    "\n",
    "normalized_X = (X - mean) / std_deviation\n",
    "print(normalized_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras import utils as np_utils\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "one_hot_labels = np_utils.to_categorical(Y, num_classes=classes+1)\n",
    "print(one_hot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    # MultiLayer Perceptron\n",
    "    model = Sequential()\n",
    "\n",
    "    # 3000 0.6 1500 0.6 -> 57% acc\n",
    "    # 2560 0.7 1280 0.7 -> cnn_model = cnn_train()\n",
    "# test_cnn(cnn_model)60% acc\n",
    "    # 2560 0.7 1600 0.7 -> 51% acc\n",
    "    # 2560 0.6 1600 0.7 -> 49%\n",
    "    # 2560 0.7 1280 -   -> 50\n",
    "    # 3000 0.7 1280 0.7 -> 54% acc\n",
    "    # 2560 0.7 1280 0.8 -> 45% acc\n",
    "\n",
    "    model.add(Dense(3000, activation='tanh', input_dim=no_of_columns))\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "#     model.add(Dense(2560, activation='tanh', input_dim=no_of_columns))\n",
    "#     model.add(Dropout(0.7))\n",
    "#     model.add(Dense(1280, activation='tanh'))\n",
    "#     model.add(Dropout(0.7))\n",
    "#     65\n",
    "    model.add(Dense(classes+1, activation='softmax'))\n",
    "\n",
    "    sgd = SGD(lr=0.01, decay=3e-5, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(normalized_X, one_hot_labels, epochs=20, batch_size=32)\n",
    "    \n",
    "    return model\n",
    "# score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    \n",
    "    test_model = get_feature_vectors(\"test\")\n",
    "    print(test_model.shape)\n",
    "\n",
    "    test_X = numpy.copy(test_model[:, :no_of_columns])\n",
    "#     print(test_X.shape)\n",
    "\n",
    "    normalized_test_X = (test_X - mean) / std_deviation\n",
    "#     print(normalized_test_X.shape)\n",
    "\n",
    "    test_Y = numpy.copy(test_model[:, no_of_columns:])\n",
    "#     print(test_Y.shape)\n",
    "    test_labels = np_utils.to_categorical(test_Y, num_classes=classes+1)\n",
    "\n",
    "#     print(model.test_on_batch(test_X, test_labels, sample_weight=None))\n",
    "#     print(model.metrics_names)\n",
    "    predictions = model.predict(normalized_test_X)\n",
    "    \n",
    "    b = [sum(predictions[current: current+test_frames]) for current in range(0, len(predictions), test_frames)]\n",
    "    predicted_Y = []\n",
    "    for row in b:\n",
    "        predicted_Y.append(row.argmax(axis=0))\n",
    "    \n",
    "    for t, p in zip(test_Y[::test_frames].T[0], predicted_Y):\n",
    "        print (int(t), p)\n",
    "    \n",
    "    diff = predicted_Y - test_Y[::test_frames].T[0]\n",
    "    correct = sum(x == 0 for x in diff)\n",
    "    total = len(predicted_Y)\n",
    "    percent = correct/total * 100\n",
    "    print(str(correct) + \" / \" + str(total) + \" = \" + str(percent))\n",
    "    \n",
    "    loss = str(numpy.sum(numpy.abs(diff)))\n",
    "    print(\"Loss = \" + loss)\n",
    "    \n",
    "    return correct, percent, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7200/7200 [==============================] - 5s 717us/step - loss: 0.9135 - acc: 0.6663\n",
      "Epoch 2/20\n",
      "7200/7200 [==============================] - 5s 675us/step - loss: 0.5651 - acc: 0.7910\n",
      "Epoch 3/20\n",
      "7200/7200 [==============================] - 5s 650us/step - loss: 0.4947 - acc: 0.8197\n",
      "Epoch 4/20\n",
      "7200/7200 [==============================] - 5s 656us/step - loss: 0.4323 - acc: 0.8457\n",
      "Epoch 5/20\n",
      "7200/7200 [==============================] - 5s 651us/step - loss: 0.4200 - acc: 0.8465\n",
      "Epoch 6/20\n",
      "7200/7200 [==============================] - 5s 671us/step - loss: 0.3863 - acc: 0.8636\n",
      "Epoch 7/20\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.3417 - acc: 0.8824\n",
      "Epoch 8/20\n",
      "7200/7200 [==============================] - 7s 960us/step - loss: 0.3365 - acc: 0.8826\n",
      "Epoch 9/20\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.3102 - acc: 0.8918\n",
      "Epoch 10/20\n",
      "7200/7200 [==============================] - 7s 988us/step - loss: 0.2955 - acc: 0.8992\n",
      "Epoch 11/20\n",
      "7200/7200 [==============================] - 6s 885us/step - loss: 0.2793 - acc: 0.9012\n",
      "Epoch 12/20\n",
      "7200/7200 [==============================] - 5s 738us/step - loss: 0.2773 - acc: 0.9047\n",
      "Epoch 13/20\n",
      "7200/7200 [==============================] - 6s 891us/step - loss: 0.2616 - acc: 0.9137\n",
      "Epoch 14/20\n",
      "7200/7200 [==============================] - 6s 813us/step - loss: 0.2401 - acc: 0.9190\n",
      "Epoch 15/20\n",
      "7200/7200 [==============================] - 6s 806us/step - loss: 0.2405 - acc: 0.9204\n",
      "Epoch 16/20\n",
      "7200/7200 [==============================] - 6s 824us/step - loss: 0.2483 - acc: 0.9174\n",
      "Epoch 17/20\n",
      "7200/7200 [==============================] - 5s 711us/step - loss: 0.2132 - acc: 0.9261\n",
      "Epoch 18/20\n",
      "7200/7200 [==============================] - 5s 731us/step - loss: 0.2060 - acc: 0.9283\n",
      "Epoch 19/20\n",
      "7200/7200 [==============================] - 5s 761us/step - loss: 0.2058 - acc: 0.9282\n",
      "Epoch 20/20\n",
      "7200/7200 [==============================] - 5s 759us/step - loss: 0.2080 - acc: 0.9315\n",
      "SR :44100 M001_rohan_6.wav\n",
      "SR :44100 F002_ritu_5.wav\n",
      "SR :8000 F004_aditi_2.wav\n",
      "SR :8000 M004_sudesh_5.wav\n",
      "SR :44100 F002_ritu_6.wav\n",
      "SR :8000 F003_shraddha_2.wav\n",
      "SR :8000 M002_shubham_8.wav\n",
      "SR :8000 F003_shraddha_5.wav\n",
      "SR :44100 F001_srishti_5.wav\n",
      "SR :44100 F001_srishti_1.wav\n",
      "SR :8000 M002_shubham_7.wav\n",
      "SR :8000 M005_abhinav_1.wav\n",
      "SR :44100 M001_rohan_3.wav\n",
      "SR :8000 M005_abhinav_7.wav\n",
      "SR :8000 M004_sudesh_1.wav\n",
      "SR :44100 F001_srishti_7.wav\n",
      "SR :44100 F001_srishti_8.wav\n",
      "SR :8000 F003_shraddha_4.wav\n",
      "SR :8000 F004_aditi_6.wav\n",
      "SR :44100 F001_srishti_3.wav\n",
      "SR :8000 M004_sudesh_4.wav\n",
      "SR :8000 F004_aditi_7.wav\n",
      "SR :8000 F004_aditi_1.wav\n",
      "SR :44100 F002_ritu_3.wav\n",
      "SR :44100 M001_rohan_1.wav\n",
      "SR :44100 F002_ritu_4.wav\n",
      "SR :8000 M004_sudesh_6.wav\n",
      "SR :8000 M005_abhinav_3.wav\n",
      "SR :8000 M002_shubham_4.wav\n",
      "SR :8000 F003_shraddha_3.wav\n",
      "SR :8000 M002_shubham_5.wav\n",
      "SR :44100 F002_ritu_1.wav\n",
      "SR :8000 M005_abhinav_6.wav\n",
      "SR :44100 M001_rohan_8.wav\n",
      "SR :8000 F003_shraddha_6.wav\n",
      "SR :44100 M001_rohan_7.wav\n",
      "SR :8000 F004_aditi_3.wav\n",
      "SR :44100 M003_punit_3.wav\n",
      "SR :8000 M002_shubham_3.wav\n",
      "SR :8000 M002_shubham_1.wav\n",
      "SR :8000 F003_shraddha_1.wav\n",
      "SR :8000 M002_shubham_2.wav\n",
      "SR :8000 F004_aditi_5.wav\n",
      "SR :44100 F001_srishti_2.wav\n",
      "SR :8000 F004_aditi_8.wav\n",
      "SR :8000 M004_sudesh_3.wav\n",
      "SR :44100 M003_punit_2.wav\n",
      "SR :8000 F003_shraddha_8.wav\n",
      "SR :44100 M001_rohan_2.wav\n",
      "SR :44100 F001_srishti_6.wav\n",
      "SR :8000 M005_abhinav_2.wav\n",
      "SR :8000 F004_aditi_4.wav\n",
      "SR :8000 M004_sudesh_8.wav\n",
      "SR :8000 M005_abhinav_5.wav\n",
      "SR :8000 M005_abhinav_8.wav\n",
      "SR :8000 M002_shubham_6.wav\n",
      "SR :8000 M004_sudesh_2.wav\n",
      "SR :44100 F001_srishti_4.wav\n",
      "SR :44100 F002_ritu_2.wav\n",
      "SR :8000 F003_shraddha_7.wav\n",
      "SR :44100 M001_rohan_5.wav\n",
      "SR :8000 M005_abhinav_4.wav\n",
      "SR :44100 M001_rohan_4.wav\n",
      "SR :8000 M004_sudesh_7.wav\n",
      "SR :44100 M003_punit_4.wav\n",
      "SR :44100 M003_punit_1.wav\n",
      "(3300, 53)\n",
      "6 6\n",
      "2 1\n",
      "4 4\n",
      "9 9\n",
      "2 1\n",
      "3 7\n",
      "7 9\n",
      "3 3\n",
      "1 1\n",
      "1 2\n",
      "7 7\n",
      "10 10\n",
      "6 6\n",
      "10 10\n",
      "9 10\n",
      "1 1\n",
      "1 1\n",
      "3 3\n",
      "4 4\n",
      "1 1\n",
      "9 7\n",
      "4 4\n",
      "4 4\n",
      "2 1\n",
      "6 6\n",
      "2 1\n",
      "9 10\n",
      "10 10\n",
      "7 7\n",
      "3 3\n",
      "7 7\n",
      "2 1\n",
      "10 10\n",
      "6 6\n",
      "3 4\n",
      "6 6\n",
      "4 4\n",
      "8 8\n",
      "7 10\n",
      "7 9\n",
      "3 4\n",
      "7 10\n",
      "4 4\n",
      "1 1\n",
      "4 4\n",
      "9 7\n",
      "8 8\n",
      "3 3\n",
      "6 6\n",
      "1 1\n",
      "10 10\n",
      "4 4\n",
      "9 10\n",
      "10 10\n",
      "10 7\n",
      "7 7\n",
      "9 7\n",
      "1 1\n",
      "2 1\n",
      "3 3\n",
      "6 6\n",
      "10 10\n",
      "6 6\n",
      "9 9\n",
      "8 8\n",
      "8 8\n",
      "45 / 66 = 68.1818181818\n",
      "Loss = 35.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(45, 68.181818181818173, '35.0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = train()\n",
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_train():\n",
    "    \n",
    "    temp = normalized_X.reshape(normalized_X.shape[0], no_of_columns, 1)\n",
    "    \n",
    "    model = Sequential()\n",
    "    # 13 7 1 1 0.25 60 0.25 10 - 70%\n",
    "    \n",
    "    model.add(Convolution1D(52, 13, activation='tanh', input_shape=(no_of_columns,1)))\n",
    "    print(model.output_shape)\n",
    "    model.add(Convolution1D(52, 7, activation='tanh'))\n",
    "    print(model.output_shape)\n",
    "    model.add(Convolution1D(13, 3, activation='tanh'))\n",
    "    print(model.output_shape)\n",
    "    \n",
    "    # stride = 2 - 70\n",
    "    # 20, 10, 17 op - 64\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size=(1)))\n",
    "#     model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1000, activation='tanh'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # 0.4 70\n",
    "    model.add(Dense(classes+1, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(temp, one_hot_labels, epochs=20, batch_size=100)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_cnn(model):\n",
    "    \n",
    "    test_model = get_feature_vectors(\"test\")\n",
    "#     print(test_model.shape)\n",
    "\n",
    "    test_X = numpy.copy(test_model[:, :no_of_columns])\n",
    "#     print(test_X.shape)\n",
    "\n",
    "    normalized_test_X = (test_X - mean) / std_deviation\n",
    "#     print(normalized_test_X.shape)\n",
    "\n",
    "    test_Y = numpy.copy(test_model[:, no_of_columns:])\n",
    "#     print(test_Y.shape)\n",
    "    test_labels = np_utils.to_categorical(test_Y, num_classes=classes+1)\n",
    "    \n",
    "    test_X = test_X.reshape(test_X.shape[0], no_of_columns, 1)\n",
    "    normalized_test_X = normalized_test_X.reshape(normalized_test_X.shape[0], no_of_columns, 1)\n",
    "    \n",
    "    print(model.test_on_batch(normalized_test_X, test_labels, sample_weight=None))\n",
    "    print(model.metrics_names)\n",
    "    predictions = model.predict(normalized_test_X)\n",
    "\n",
    "    b = [sum(predictions[current: current+test_frames]) for current in range(0, len(predictions), test_frames)]\n",
    "    predicted_Y = []\n",
    "    for row in b:\n",
    "        predicted_Y.append(row.argmax(axis=0))\n",
    "\n",
    "    # print(predicted_Y)\n",
    "    # print(test_Y[::40].T)\n",
    "    \n",
    "    indices = numpy.argmax(predictions, axis=1)\n",
    "    majority = []\n",
    "    \n",
    "    for i in range(0, len(indices), test_frames):\n",
    "        majority.append(find_majority(indices[i:i + test_frames]))\n",
    "        \n",
    "#     majority = \n",
    "    for t, p, m in zip(test_Y[::test_frames].T[0], predicted_Y, majority):\n",
    "        print(int(t), p, m[0])\n",
    "    \n",
    "#     for t, p in zip(test_Y.T[0], indices):\n",
    "#         print(int(t), p)\n",
    "    \n",
    "    \n",
    "    diff = predicted_Y - test_Y[::test_frames].T[0]\n",
    "    maj_diff = numpy.array(majority)[:, 0] - test_Y[::test_frames].T[0]\n",
    "\n",
    "    numerator = sum(x == 0 for x in diff)\n",
    "    denominator = len(predicted_Y)\n",
    "    \n",
    "    numerator2 = sum(x == 0 for x in maj_diff)\n",
    "    denominator2 = len(maj_diff)\n",
    "    \n",
    "\n",
    "    print(\"Accuracy prob_diff: {} of {} - {}\".format(numerator, denominator, numerator/denominator))\n",
    "    \n",
    "    print(\"Accuracy maj_diff: {} of {} - {}\".format(numerator2, denominator2, numerator2/denominator2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 40, 52)\n",
      "(None, 34, 52)\n",
      "(None, 32, 13)\n",
      "Epoch 1/20\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 1.8397 - acc: 0.3850\n",
      "Epoch 2/20\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.4551 - acc: 0.5033\n",
      "Epoch 3/20\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.2638 - acc: 0.5686\n",
      "Epoch 4/20\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.0925 - acc: 0.6255\n",
      "Epoch 5/20\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 0.9467 - acc: 0.6731\n",
      "Epoch 6/20\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 0.8120 - acc: 0.7226\n",
      "Epoch 7/20\n",
      "24000/24000 [==============================] - 32s 1ms/step - loss: 0.6988 - acc: 0.7590\n",
      "Epoch 8/20\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 0.6056 - acc: 0.7940\n",
      "Epoch 9/20\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 0.5278 - acc: 0.8208\n",
      "Epoch 10/20\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 0.4492 - acc: 0.8472\n",
      "Epoch 11/20\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 0.3980 - acc: 0.8645\n",
      "Epoch 12/20\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 0.3499 - acc: 0.8795\n",
      "Epoch 13/20\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 0.3064 - acc: 0.8968\n",
      "Epoch 14/20\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 0.2701 - acc: 0.9093\n",
      "Epoch 15/20\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 0.2406 - acc: 0.9198\n",
      "Epoch 16/20\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 0.2169 - acc: 0.9257\n",
      "Epoch 17/20\n",
      "24000/24000 [==============================] - 32s 1ms/step - loss: 0.1976 - acc: 0.9352\n",
      "Epoch 18/20\n",
      "24000/24000 [==============================] - 32s 1ms/step - loss: 0.1816 - acc: 0.9403\n",
      "Epoch 19/20\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 0.1656 - acc: 0.9445\n",
      "Epoch 20/20\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 0.1579 - acc: 0.9476\n",
      "[array(3.004624605178833, dtype=float32), array(0.4026086926460266, dtype=float32)]\n",
      "['loss', 'acc']\n",
      "9 5 5\n",
      "0 0 0\n",
      "19 16 19\n",
      "20 20 20\n",
      "15 15 15\n",
      "2 2 3\n",
      "1 1 1\n",
      "0 3 3\n",
      "16 16 16\n",
      "8 8 8\n",
      "24 24 24\n",
      "3 3 3\n",
      "19 19 19\n",
      "4 4 4\n",
      "1 4 4\n",
      "6 6 6\n",
      "8 8 8\n",
      "4 5 5\n",
      "21 21 21\n",
      "19 19 19\n",
      "20 20 20\n",
      "1 4 4\n",
      "20 20 20\n",
      "1 1 4\n",
      "21 21 21\n",
      "5 5 5\n",
      "23 23 23\n",
      "5 5 5\n",
      "22 19 19\n",
      "21 21 21\n",
      "16 16 16\n",
      "1 2 2\n",
      "19 19 19\n",
      "21 21 21\n",
      "8 8 8\n",
      "23 23 23\n",
      "22 22 22\n",
      "24 24 24\n",
      "8 8 8\n",
      "23 17 17\n",
      "21 21 21\n",
      "6 6 6\n",
      "1 7 7\n",
      "3 3 3\n",
      "19 19 19\n",
      "5 5 5\n",
      "20 20 20\n",
      "16 16 16\n",
      "9 3 3\n",
      "20 18 18\n",
      "0 0 0\n",
      "16 16 16\n",
      "21 21 21\n",
      "15 15 15\n",
      "16 16 16\n",
      "24 24 24\n",
      "3 3 3\n",
      "23 20 20\n",
      "5 2 2\n",
      "16 16 16\n",
      "0 0 0\n",
      "20 18 18\n",
      "3 3 3\n",
      "5 2 2\n",
      "0 0 0\n",
      "6 4 4\n",
      "19 19 19\n",
      "19 19 19\n",
      "21 21 21\n",
      "0 8 8\n",
      "2 5 5\n",
      "4 5 5\n",
      "21 21 21\n",
      "20 20 20\n",
      "7 5 5\n",
      "21 21 21\n",
      "23 23 23\n",
      "5 5 5\n",
      "1 1 1\n",
      "1 1 1\n",
      "23 21 21\n",
      "16 16 16\n",
      "9 9 9\n",
      "7 7 5\n",
      "0 0 0\n",
      "20 18 18\n",
      "19 19 19\n",
      "8 8 8\n",
      "23 17 17\n",
      "2 2 2\n",
      "15 23 23\n",
      "17 17 17\n",
      "9 9 9\n",
      "21 21 21\n",
      "3 3 3\n",
      "4 2 6\n",
      "3 3 3\n",
      "15 15 15\n",
      "20 20 20\n",
      "1 5 5\n",
      "22 19 19\n",
      "1 2 2\n",
      "21 21 21\n",
      "3 3 3\n",
      "9 9 9\n",
      "20 20 20\n",
      "17 17 17\n",
      "19 19 19\n",
      "19 19 19\n",
      "2 4 4\n",
      "6 6 6\n",
      "1 2 2\n",
      "3 3 3\n",
      "1 1 1\n",
      "20 21 20\n",
      "3 3 3\n",
      "20 18 18\n",
      "5 2 2\n",
      "4 5 5\n",
      "7 0 0\n",
      "20 20 20\n",
      "6 3 3\n",
      "0 0 0\n",
      "15 15 15\n",
      "22 19 19\n",
      "1 3 3\n",
      "1 1 1\n",
      "19 19 19\n",
      "20 20 20\n",
      "23 18 18\n",
      "5 5 5\n",
      "20 18 18\n",
      "22 22 22\n",
      "23 17 17\n",
      "6 3 3\n",
      "1 2 2\n",
      "21 21 21\n",
      "24 24 24\n",
      "1 2 2\n",
      "4 5 5\n",
      "20 18 18\n",
      "4 0 0\n",
      "16 16 16\n",
      "1 3 3\n",
      "1 3 3\n",
      "6 3 3\n",
      "21 21 21\n",
      "22 22 22\n",
      "7 9 9\n",
      "1 5 5\n",
      "5 5 5\n",
      "7 7 7\n",
      "0 0 0\n",
      "3 1 1\n",
      "23 23 23\n",
      "19 19 19\n",
      "1 4 4\n",
      "19 19 19\n",
      "1 5 5\n",
      "3 3 3\n",
      "19 16 16\n",
      "5 5 5\n",
      "21 21 21\n",
      "16 16 16\n",
      "19 16 20\n",
      "24 24 24\n",
      "21 21 21\n",
      "3 3 3\n",
      "20 20 20\n",
      "5 5 5\n",
      "2 5 5\n",
      "19 19 19\n",
      "5 5 5\n",
      "20 20 20\n",
      "20 20 20\n",
      "19 19 19\n",
      "19 19 19\n",
      "21 21 21\n",
      "20 23 23\n",
      "9 9 9\n",
      "16 16 16\n",
      "20 20 20\n",
      "4 4 4\n",
      "19 19 19\n",
      "4 5 4\n",
      "0 0 0\n",
      "1 5 5\n",
      "5 5 5\n",
      "4 4 4\n",
      "22 19 19\n",
      "17 17 17\n",
      "1 3 3\n",
      "17 17 17\n",
      "16 16 16\n",
      "3 3 3\n",
      "20 20 20\n",
      "1 1 1\n",
      "22 19 19\n",
      "7 3 3\n",
      "0 0 0\n",
      "18 18 18\n",
      "15 15 15\n",
      "22 22 22\n",
      "9 9 9\n",
      "21 21 21\n",
      "3 5 6\n",
      "2 5 7\n",
      "20 19 19\n",
      "17 17 17\n",
      "0 0 0\n",
      "3 3 3\n",
      "20 20 20\n",
      "21 21 21\n",
      "20 20 20\n",
      "7 5 5\n",
      "21 21 21\n",
      "8 3 3\n",
      "19 19 19\n",
      "0 2 2\n",
      "3 3 3\n",
      "1 1 1\n",
      "19 19 19\n",
      "17 17 17\n",
      "1 5 5\n",
      "6 5 5\n",
      "6 3 3\n",
      "1 1 1\n",
      "20 20 20\n",
      "1 3 3\n",
      "5 5 5\n",
      "21 21 21\n",
      "8 8 8\n",
      "9 5 3\n",
      "3 3 3\n",
      "21 21 21\n",
      "2 2 2\n",
      "18 18 18\n",
      "3 8 8\n",
      "5 2 2\n",
      "21 21 21\n",
      "1 0 0\n",
      "6 3 3\n",
      "3 3 3\n",
      "23 21 21\n",
      "19 19 19\n",
      "19 15 15\n",
      "20 20 20\n",
      "23 24 24\n",
      "18 18 18\n",
      "1 1 1\n",
      "5 5 5\n",
      "8 8 8\n",
      "6 6 6\n",
      "Accuracy prob_diff: 169 of 253 - 0.6679841897233202\n",
      "Accuracy maj_diff: 169 of 253 - 0.6679841897233202\n"
     ]
    }
   ],
   "source": [
    "cnn_model = cnn_train()\n",
    "test_cnn(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dense_cnn_train():\n",
    "    \n",
    "    model = Sequential()\n",
    "    # 13 7 1 1 0.25 60 0.25 10 - 70%\n",
    "    \n",
    "    model.add(Dense(2560, activation='tanh', input_dim=no_of_columns))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1300, activation='tanh'))\n",
    "\n",
    "    model.add(Reshape((52, -1)))\n",
    "    model.add(Convolution1D(52, 13, activation='tanh'))\n",
    "    model.add(Convolution1D(52, 7, activation='tanh'))\n",
    "    model.add(Convolution1D(13, 3, activation='tanh'))\n",
    "    \n",
    "    # stride = 2 - 70\n",
    "    # 20, 10, 17 op - 64\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size=(1)))\n",
    "#     model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1000, activation='tanh'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # 0.4 70\n",
    "    model.add(Dense(classes+1, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(normalized_X, one_hot_labels, epochs=10, batch_size=100)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dense_cnn_test(model):\n",
    "    \n",
    "    test_model = get_feature_vectors(\"test\")\n",
    "#     print(test_model.shape)\n",
    "\n",
    "    test_X = numpy.copy(test_model[:, :no_of_columns])\n",
    "#     print(test_X.shape)\n",
    "\n",
    "    normalized_test_X = (test_X - mean) / std_deviation\n",
    "#     print(normalized_test_X.shape)\n",
    "\n",
    "    test_Y = numpy.copy(test_model[:, no_of_columns:])\n",
    "#     print(test_Y.shape)\n",
    "    test_labels = np_utils.to_categorical(test_Y, num_classes=classes+1)\n",
    "    \n",
    "#     test_X = test_X.reshape(test_X.shape[0], no_of_columns, 1)\n",
    "#     normalized_test_X = normalized_test_X.reshape(normalized_test_X.shape[0], no_of_columns, 1)\n",
    "    \n",
    "    print(model.test_on_batch(normalized_test_X, test_labels, sample_weight=None))\n",
    "    print(model.metrics_names)\n",
    "    predictions = model.predict(normalized_test_X)\n",
    "\n",
    "    b = [sum(predictions[current: current+test_frames]) for current in range(0, len(predictions), test_frames)]\n",
    "    predicted_Y = []\n",
    "    for row in b:\n",
    "        predicted_Y.append(row.argmax(axis=0))\n",
    "\n",
    "    # print(predicted_Y)\n",
    "    # print(test_Y[::40].T)\n",
    "    \n",
    "    indices = numpy.argmax(predictions, axis=1)\n",
    "    majority = []\n",
    "    \n",
    "    for i in range(0, len(indices), test_frames):\n",
    "        majority.append(find_majority(indices[i:i + test_frames]))\n",
    "        \n",
    "#     majority = \n",
    "    for t, p, m in zip(test_Y[::test_frames].T[0], predicted_Y, majority):\n",
    "        print(int(t), p, m[0])\n",
    "    \n",
    "#     for t, p in zip(test_Y.T[0], indices):\n",
    "#         print(int(t), p)\n",
    "    \n",
    "    \n",
    "    diff = predicted_Y - test_Y[::test_frames].T[0]\n",
    "    maj_diff = numpy.array(majority)[:, 0] - test_Y[::test_frames].T[0]\n",
    "\n",
    "    numerator = sum(x == 0 for x in diff)\n",
    "    denominator = len(predicted_Y)\n",
    "    \n",
    "    numerator2 = sum(x == 0 for x in maj_diff)\n",
    "    denominator2 = len(maj_diff)\n",
    "    \n",
    "\n",
    "    print(\"Accuracy prob_diff: {} of {} - {}\".format(numerator, denominator, numerator/denominator))\n",
    "    \n",
    "    print(\"Accuracy maj_diff: {} of {} - {}\".format(numerator2, denominator2, numerator2/denominator2))\n",
    "    \n",
    "\n",
    "# print(predicted_Y)\n",
    "# print(test_Y[::40].T)\n",
    "\n",
    "# for t, p in zip(test_Y[::40].T[0], predicted_Y):\n",
    "#     print (int(t), p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4800/4800 [==============================] - 21s 4ms/step - loss: 0.4361 - acc: 0.8348\n",
      "Epoch 2/10\n",
      "4800/4800 [==============================] - 19s 4ms/step - loss: 0.1857 - acc: 0.9288\n",
      "Epoch 3/10\n",
      "4800/4800 [==============================] - 18s 4ms/step - loss: 0.1448 - acc: 0.9481\n",
      "Epoch 4/10\n",
      "3700/4800 [======================>.......] - ETA: 4s - loss: 0.0939 - acc: 0.9646"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e152fe2daed8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdense_cnn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdense_cnn_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdense_cnn_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_cnn_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-34648f2e218b>\u001b[0m in \u001b[0;36mdense_cnn_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/theano/ifelse.py\u001b[0m in \u001b[0;36mthunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mthunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dense_cnn_model = dense_cnn_train()\n",
    "dense_cnn_test(dense_cnn_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_majority(k):\n",
    "    myMap = {}\n",
    "    maximum = ( '', 0 ) # (occurring element, occurrences)\n",
    "    for n in k:\n",
    "        if n in myMap: myMap[n] += 1\n",
    "        else: myMap[n] = 1\n",
    "\n",
    "        # Keep track of maximum on the go\n",
    "        if myMap[n] > maximum[1]: maximum = (n,myMap[n])\n",
    "\n",
    "    return maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try removing lowpass for lower sample rate\n",
    "# w/o punit - 8 speakers\n",
    "# 67 67 70 dense\n",
    "# 80 74 74 cnn \n",
    "# 74 77 77 dense_cnn\n",
    "\n",
    "# with punit - up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
