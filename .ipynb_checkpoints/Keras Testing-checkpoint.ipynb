{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features import delta\n",
    "from python_speech_features import logfbank\n",
    "import scipy.io.wavfile as wav\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_of_features = 20\n",
    "no_of_frames = 25\n",
    "\n",
    "no_of_columns = 3 * no_of_features * no_of_frames\n",
    "\n",
    "def get_feature_vectors(dataset_type):\n",
    "    \n",
    "    #set parameters for training and testing\n",
    "    if (dataset_type == \"train\"):\n",
    "        directory = os.path.join(os.getcwd(), 'data_thuyg20_sre/enroll')\n",
    "#         no_of_frames = 40\n",
    "        no_of_cycles = 80\n",
    "    elif (dataset_type == \"test\"):    \n",
    "        directory = os.path.join(os.getcwd(), 'data_thuyg20_sre/test')\n",
    "#         no_of_frames = 40\n",
    "        no_of_cycles = 1\n",
    "    \n",
    "    dataset = numpy.empty([0, no_of_columns + 1])\n",
    "    stats_dataset = numpy.empty([0, 3 * no_of_features])\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        \n",
    "        # filter speakers\n",
    "        names = ['F101', 'F102', 'F103', 'F104', 'F105', 'M101', 'M102', 'M103']\n",
    "\n",
    "        if any(name in file for name in names):\n",
    "            \n",
    "            # extract mfcc vectors\n",
    "            (rate,sig) = wav.read(os.path.join(directory, file))\n",
    "            mfcc_feat = mfcc(sig,rate)\n",
    "            d_mfcc_feat = delta(mfcc_feat, 2)\n",
    "            dd_mfcc_feat = delta(d_mfcc_feat, 2)\n",
    "            \n",
    "            for x in range(0, no_of_cycles):\n",
    "                \n",
    "                random_int = randint(1, 300)\n",
    "\n",
    "                fbank_feat = logfbank(sig,rate)\n",
    "                mfcc_vectors = mfcc_feat[random_int:random_int+no_of_frames, :no_of_features]\n",
    "                dmfcc_vectors = d_mfcc_feat[random_int:random_int+no_of_frames, :no_of_features]\n",
    "#                 ddmfcc_vectors = dd_mfcc_feat[random_int:random_int+no_of_frames, :no_of_features]\n",
    "                fbank_vectors = fbank_feat[random_int:random_int+no_of_frames, :no_of_features]\n",
    "                \n",
    "                feature_vectors = numpy.hstack((mfcc_vectors, dmfcc_vectors, fbank_vectors))\n",
    "                \n",
    "                stats_dataset = numpy.concatenate((stats_dataset, feature_vectors), axis=0)\n",
    "                \n",
    "                feature_vectors = feature_vectors.flatten().reshape((1, -1))\n",
    "\n",
    "                # get speaker index from filename\n",
    "                speaker_index = file.split(\"_\")[0]\n",
    "                if speaker_index[0] == 'M':\n",
    "                    speaker_index = 5 + int(speaker_index[3:])\n",
    "                else:\n",
    "                    speaker_index = int(speaker_index[3:])\n",
    "\n",
    "                #append speaker index to feature vectors\n",
    "                np_speaker_index = numpy.array([speaker_index])\n",
    "                temp = numpy.tile(np_speaker_index[numpy.newaxis,:], (feature_vectors.shape[0],1))\n",
    "                concatenated_feature_vector = numpy.concatenate((feature_vectors,temp), axis=1)\n",
    "\n",
    "#                 print(concatenated_feature_vector.shape)\n",
    "                # append file's data to dataset\n",
    "                dataset = numpy.concatenate((dataset, concatenated_feature_vector), axis=0)\n",
    "\n",
    "    \n",
    "    mean = stats_dataset.mean(0, keepdims=True)\n",
    "    std_deviation = numpy.std(stats_dataset, axis=0, keepdims=True)\n",
    "    \n",
    "    if (dataset_type == \"train\"):\n",
    "        return dataset, mean, std_deviation\n",
    "    elif (dataset_type == \"test\"):    \n",
    "        return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_feature_vectors(dataset_type):\n",
    "    \n",
    "#     #set parameters for training and testing\n",
    "#     if (dataset_type == \"train\"):\n",
    "#         directory = os.path.join(os.getcwd(), 'data_thuyg20_sre/enroll')\n",
    "#         no_of_frames = 40\n",
    "#     elif (dataset_type == \"test\"):    \n",
    "#         directory = os.path.join(os.getcwd(), 'data_thuyg20_sre/test')\n",
    "#         no_of_frames = 40\n",
    "    \n",
    "#     dataset = numpy.empty([0, 61])\n",
    "    \n",
    "#     for file in os.listdir(directory):\n",
    "        \n",
    "#         # filter speakers\n",
    "#         names = ['F101', 'F102', 'F103', 'F104', 'F105', 'M101', 'M102', 'M103', 'M104']\n",
    "\n",
    "#         if any(name in file for name in names):\n",
    "            \n",
    "#             # extract mfcc vectors\n",
    "#             (rate,sig) = wav.read(os.path.join(directory, file))\n",
    "#             mfcc_feat = mfcc(sig,rate)\n",
    "#             d_mfcc_feat = delta(mfcc_feat, 2)\n",
    "#             dd_mfcc_feat = delta(d_mfcc_feat, 2)\n",
    "            \n",
    "# #             fbank_feat = logfbank(sig,rate)\n",
    "#             mfcc_vectors = mfcc_feat[11:11+no_of_frames,:]\n",
    "#             dmfcc_vectors = d_mfcc_feat[11:11+no_of_frames,:]\n",
    "#             ddmfcc_vectors = dd_mfcc_feat[11:11+no_of_frames,:]\n",
    "            \n",
    "#             feature_vectors = numpy.hstack((mfcc_vectors, dmfcc_vectors, ddmfcc_vectors))\n",
    "# #             print(feature_vectors.shape)\n",
    "            \n",
    "#             # get speaker index from filename\n",
    "#             speaker_index = file.split(\"_\")[0]\n",
    "#             if speaker_index[0] == 'M':\n",
    "#                 speaker_index = 5 + int(speaker_index[3:])\n",
    "#             else:\n",
    "#                 speaker_index = int(speaker_index[3:])\n",
    "\n",
    "#             #append speaker index to feature vectors\n",
    "#             np_speaker_index = numpy.array([speaker_index])\n",
    "#             temp = numpy.tile(np_speaker_index[numpy.newaxis,:], (feature_vectors.shape[0],1))\n",
    "#             concatenated_feature_vector = numpy.concatenate((feature_vectors,temp), axis=1)\n",
    "            \n",
    "#             # append file's data to dataset\n",
    "#             dataset = numpy.concatenate((dataset, concatenated_feature_vector), axis=0)\n",
    "            \n",
    "\n",
    "#     return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "import numpy as numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import genfromtxt\n",
    "my_data, mean, std_deviation = get_feature_vectors(\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 1501)\n"
     ]
    }
   ],
   "source": [
    "# print(my_data)\n",
    "print(my_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 1)\n"
     ]
    }
   ],
   "source": [
    "Y = numpy.copy(my_data[:, no_of_columns:])\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 1500)\n",
      "(1, 1500)\n",
      "(1, 1500)\n",
      "(640, 1500)\n"
     ]
    }
   ],
   "source": [
    "X = numpy.copy(my_data[:, :no_of_columns])\n",
    "print(X.shape)\n",
    "\n",
    "mean = numpy.tile(mean, no_of_frames)\n",
    "print(mean.shape)\n",
    "\n",
    "std_deviation = numpy.tile(std_deviation, no_of_frames)\n",
    "print(std_deviation.shape)\n",
    "\n",
    "normalized_X = (X - mean) / std_deviation\n",
    "print(normalized_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 10)\n"
     ]
    }
   ],
   "source": [
    "from keras import utils as np_utils\n",
    "\n",
    "one_hot_labels = np_utils.to_categorical(Y, num_classes=10)\n",
    "print(one_hot_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 1.5453 - acc: 0.4969\n",
      "Epoch 2/10\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3489 - acc: 0.9141\n",
      "Epoch 3/10\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.1379 - acc: 0.9719\n",
      "Epoch 4/10\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.0778 - acc: 0.9797\n",
      "Epoch 5/10\n",
      "640/640 [==============================] - 2s 4ms/step - loss: 0.0665 - acc: 0.9797\n",
      "Epoch 6/10\n",
      "640/640 [==============================] - 3s 5ms/step - loss: 0.0491 - acc: 0.9844\n",
      "Epoch 7/10\n",
      "640/640 [==============================] - 3s 5ms/step - loss: 0.0445 - acc: 0.9859\n",
      "Epoch 8/10\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.0551 - acc: 0.9797\n",
      "Epoch 9/10\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.0400 - acc: 0.9859\n",
      "Epoch 10/10\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.0373 - acc: 0.9859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5d73623d30>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(32, activation='relu', input_dim=12))\n",
    "# model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='rmsprop',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit(normalized_X, one_hot_labels, epochs=10, batch_size=32)\n",
    "\n",
    "\n",
    "# MultiLayer Perceptron\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1000, activation='tanh', input_dim=no_of_columns))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(1000, activation='tanh', use_bias=True))\n",
    "model.add(Dense(900, activation='tanh', use_bias=True))\n",
    "# # model.add(Dropout(0.5))\n",
    "# model.add(Dense(800, activation='tanh', use_bias=True))\n",
    "model.add(Dense(600, activation='tanh', use_bias=True))\n",
    "model.add(Dense(400, activation='tanh', use_bias=True))\n",
    "# model.add(Dense(400, activation='tanh', use_bias=True))\n",
    "# model.add(Dense(4000, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(4000, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(4000, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(normalized_X, one_hot_labels, epochs=10, batch_size=32)\n",
    "# score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 1501)\n",
      "(70, 1500)\n",
      "(70, 1500)\n",
      "(70, 1)\n"
     ]
    }
   ],
   "source": [
    "test_model = get_feature_vectors(\"test\")\n",
    "print(test_model.shape)\n",
    "\n",
    "test_X = numpy.copy(test_model[:, :no_of_columns])\n",
    "print(test_X.shape)\n",
    "\n",
    "normalized_test_X = (test_X - mean) / std_deviation\n",
    "print(normalized_test_X.shape)\n",
    "\n",
    "test_Y = numpy.copy(test_model[:, no_of_columns:])\n",
    "print(test_Y.shape)\n",
    "test_labels = np_utils.to_categorical(test_Y, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(2.2090818881988525, dtype=float32), array(0.4571428596973419, dtype=float32)]\n",
      "['loss', 'acc']\n",
      "[1 1 6 5 1 4 2 1 5 3 1 1 3 6 3 2 5 1 2 2 5 6 2 7 1 3 1 3 2 1 3 5 1 1 1 6 1\n",
      " 3 7 3 1 1 1 2 8 1 1 1 1 1 1 3 1 1 8 1 1 3 2 7 8 5 7 1 1 5 5 5 8 1]\n"
     ]
    }
   ],
   "source": [
    "print(model.test_on_batch(test_X, test_labels, sample_weight=None))\n",
    "print(model.metrics_names)\n",
    "predictions = model.predict(test_X).argmax(axis=1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# b = [sum(predictions[current: current+40]) for current in range(0, len(predictions), 40)]\n",
    "# predicted_Y = []\n",
    "# for row in b:\n",
    "#     predicted_Y.append(row.argmax(axis=0))\n",
    "    \n",
    "# # print(predicted_Y)\n",
    "# # print(test_Y[::40].T)\n",
    "\n",
    "# # for t, p in zip(test_Y[::40].T[0], predicted_Y):\n",
    "# #     print (int(t), p)\n",
    "\n",
    "# diff = predicted_Y - test_Y[::40].T[0]\n",
    "\n",
    "# print(len(predicted_Y))\n",
    "# print(sum(x == 0 for x in diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 6\n",
      "1 3\n",
      "6 3\n",
      "4 1\n",
      "1 3\n",
      "4 3\n",
      "1 1\n",
      "5 3\n",
      "5 5\n",
      "1 1\n",
      "1 3\n",
      "3 1\n",
      "6 6\n",
      "6 5\n",
      "3 3\n",
      "5 3\n",
      "3 3\n",
      "5 1\n",
      "2 3\n",
      "4 3\n",
      "5 5\n",
      "6 7\n",
      "2 2\n",
      "7 7\n",
      "4 7\n",
      "3 3\n",
      "1 1\n",
      "7 7\n",
      "1 1\n",
      "4 3\n",
      "1 1\n",
      "5 5\n",
      "1 1\n",
      "4 4\n",
      "4 2\n",
      "6 6\n",
      "1 3\n",
      "1 1\n",
      "5 3\n",
      "3 6\n",
      "1 3\n",
      "1 1\n",
      "3 3\n",
      "5 3\n",
      "6 2\n",
      "5 3\n",
      "4 5\n",
      "4 5\n",
      "1 3\n",
      "5 1\n",
      "4 1\n",
      "7 3\n",
      "1 2\n",
      "7 3\n",
      "6 7\n",
      "3 3\n",
      "8 1\n",
      "3 1\n",
      "2 1\n",
      "7 5\n",
      "3 1\n",
      "3 1\n",
      "7 7\n",
      "1 3\n",
      "1 1\n",
      "5 5\n",
      "3 3\n",
      "3 5\n",
      "8 1\n",
      "5 1\n",
      "70\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "# print(test_Y.T)\n",
    "\n",
    "for t, p in zip(test_Y.T[0], predictions):\n",
    "    print (int(t), p)\n",
    "\n",
    "diff = predictions - test_Y.T[0]\n",
    "\n",
    "print(len(predictions))\n",
    "print(sum(x == 0 for x in diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
